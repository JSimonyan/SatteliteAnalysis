{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sattelite Image Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aria Burk\n",
    "u1102905@utah.edu\n",
    "1102905\n",
    "\n",
    "Joseph Cochran \n",
    "u1193064@utah.edu\n",
    "1193064\n",
    "\n",
    "John Simonyan\n",
    "u0969548@utah.edu\n",
    "0969548"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background and Motivation:**\n",
    "\n",
    "As a group, each of us took an interest in NASA’s public APIs and the possibilities of implementing data science within the context of geoscience. Satellite imagery collected in near real-time all across the globe can be quite valuable for those who want to better understand weather patterns, geographical changes over time, disaster prediction/prevention, and many other large-scale surveys.\n",
    "\n",
    "Sattelite image anaylsis can have a plethora of pracitcal applicatons. Because sattelites can take pictures all over the surface of the planet they can capture formations and patterns in ways we can't see on the ground. In 2019 a group of researchers trained an ai to find geoglyphs, discovering new sets of Nazca lines (Becky Little,https://www.history.com/news/nazca-lines-peru-ai). This just shows one example of the potential of large scale sattelite image analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Objectives:**\n",
    "\n",
    "Listed below are some flexible objectives that we can expand on or dismiss during our project’s exploratory phase.\n",
    "Explore the metadata collected and associated with each image to find meaningful correlations between different labels and geographical coordinates or other identifiers.\n",
    "    \n",
    "- Analyze select images and develop an algorithm that determines whether the images are similar enough to contain the same subject or location.\n",
    "\n",
    "- Inversely, find different images of the same subject or location, test the previous algorithm’s accuracy, and compare it with different methodologies.\n",
    "- Test datasets against other tracked events to find correlations and potential patterns\n",
    "    \n",
    "- Train models on datasets to make predictions on a set of qualifiers based on the datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data:**\n",
    "\n",
    "By querying the \"Earth\" NASA API at https://api.nasa.gov/planetary/earth/imagery, we receive a base64 encoded png image that was taken by satellite. \n",
    "\n",
    "The query inputs are longitude, latitude, dimensions, and date the image was taken.\n",
    "\n",
    "Geo.json dataset containing latitude and longitude along with other attributes, city, country. Using these patameters we can more clearly classify the queried data.\n",
    "\n",
    "Cross referencing our image queries with locational queries to find relevent metadata to apply to our images.\n",
    "Metadata exploratory analysis can ecpand dataset very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "1. Specify what dataset we want(The geojson I found is of u.s. cities ) I.e which cities/dates to pull from\n",
    "2. iterate through geoJson making api calls with every iteration\n",
    "3. Time out iteration to not overload api. (1,000 requests per hour)\n",
    "4. save response data to files as png with appropriate classifiers to categorize images appropriately\n",
    "5. write out method to parse images to histogram or heat map\n",
    "6. plot out color ranges and heatmap ranges for all the images\n",
    "7. analyize data ranges to make some predictions or inferences\n",
    "8. Generate appropriate dataframes with images and img metadata for an easy visualization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state_id</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>population</th>\n",
       "      <th>density</th>\n",
       "      <th>source</th>\n",
       "      <th>military</th>\n",
       "      <th>incorporated</th>\n",
       "      <th>capital</th>\n",
       "      <th>timezone</th>\n",
       "      <th>ranking</th>\n",
       "      <th>zips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.6943</td>\n",
       "      <td>-73.9249</td>\n",
       "      <td>18713220</td>\n",
       "      <td>10715</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>1</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>34.1139</td>\n",
       "      <td>-118.4068</td>\n",
       "      <td>12750807</td>\n",
       "      <td>3276</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Cook</td>\n",
       "      <td>41.8373</td>\n",
       "      <td>-87.6862</td>\n",
       "      <td>8604203</td>\n",
       "      <td>4574</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miami</td>\n",
       "      <td>FL</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Miami-Dade</td>\n",
       "      <td>25.7839</td>\n",
       "      <td>-80.2102</td>\n",
       "      <td>6445545</td>\n",
       "      <td>5019</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>32.7936</td>\n",
       "      <td>-96.7662</td>\n",
       "      <td>5743938</td>\n",
       "      <td>1526</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19086</th>\n",
       "      <td>Portage Creek</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Dillingham</td>\n",
       "      <td>58.9051</td>\n",
       "      <td>-157.6695</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>America/Anchorage</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19087</th>\n",
       "      <td>Gross</td>\n",
       "      <td>NE</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>Boyd</td>\n",
       "      <td>42.9461</td>\n",
       "      <td>-98.5697</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19088</th>\n",
       "      <td>Lotsee</td>\n",
       "      <td>OK</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>Tulsa</td>\n",
       "      <td>36.1334</td>\n",
       "      <td>-96.2091</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19089</th>\n",
       "      <td>The Ranch</td>\n",
       "      <td>MN</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>Mahnomen</td>\n",
       "      <td>47.3198</td>\n",
       "      <td>-95.6952</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19090</th>\n",
       "      <td>Monowi</td>\n",
       "      <td>NE</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>Boyd</td>\n",
       "      <td>42.8307</td>\n",
       "      <td>-98.3296</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19091 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                city state_id       state       county      lat      long  \\\n",
       "0           New York       NY    New York     New York  40.6943  -73.9249   \n",
       "1        Los Angeles       CA  California  Los Angeles  34.1139 -118.4068   \n",
       "2            Chicago       IL    Illinois         Cook  41.8373  -87.6862   \n",
       "3              Miami       FL     Florida   Miami-Dade  25.7839  -80.2102   \n",
       "4             Dallas       TX       Texas       Dallas  32.7936  -96.7662   \n",
       "...              ...      ...         ...          ...      ...       ...   \n",
       "19086  Portage Creek       AK      Alaska   Dillingham  58.9051 -157.6695   \n",
       "19087          Gross       NE    Nebraska         Boyd  42.9461  -98.5697   \n",
       "19088         Lotsee       OK    Oklahoma        Tulsa  36.1334  -96.2091   \n",
       "19089      The Ranch       MN   Minnesota     Mahnomen  47.3198  -95.6952   \n",
       "19090         Monowi       NE    Nebraska         Boyd  42.8307  -98.3296   \n",
       "\n",
       "       population  density   source  military  incorporated  capital  \\\n",
       "0        18713220    10715  polygon     False          True    False   \n",
       "1        12750807     3276  polygon     False          True    False   \n",
       "2         8604203     4574  polygon     False          True    False   \n",
       "3         6445545     5019  polygon     False          True    False   \n",
       "4         5743938     1526  polygon     False          True    False   \n",
       "...           ...      ...      ...       ...           ...      ...   \n",
       "19086           3        0  polygon     False         False    False   \n",
       "19087           2        6  polygon     False          True    False   \n",
       "19088           2       39  polygon     False          True    False   \n",
       "19089           2        2  polygon     False          True    False   \n",
       "19090           1        1  polygon     False          True    False   \n",
       "\n",
       "                  timezone  ranking  zips  \n",
       "0         America/New_York        1   309  \n",
       "1      America/Los_Angeles        1   196  \n",
       "2          America/Chicago        1    85  \n",
       "3         America/New_York        1    30  \n",
       "4          America/Chicago        1   107  \n",
       "...                    ...      ...   ...  \n",
       "19086    America/Anchorage        3     1  \n",
       "19087      America/Chicago        3     1  \n",
       "19088      America/Chicago        3     1  \n",
       "19089      America/Chicago        3     1  \n",
       "19090      America/Chicago        3     1  \n",
       "\n",
       "[19091 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load csv of city coordinates\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "us_cities = pd.read_csv(\"us_cities_v2.csv\")\n",
    "us_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'slc_2020_04_26.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4790e3ff422e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# i.save('slc_2020_04_26.png')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mimage_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'slc_2020_04_26.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#img to histogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2903\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2904\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2905\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'slc_2020_04_26.png'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "#example query\n",
    "# url = \"https://api.nasa.gov/planetary/earth/imagery?lon=-111.9306&lat=40.7777&dim=.2&date=2020-04-26&api_key=II3XUxmGYfFKmyapNeXjrH3Os3ILNhQScjW6bbMT\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "#image returned \n",
    "# i = Image.open(BytesIO(response.content))\n",
    "# i\n",
    "\n",
    "#images is saved to file with city name and date taken as format\n",
    "# i.save('slc_2020_04_26.png')\n",
    "\n",
    "image_test = Image.open('slc_2020_04_26.png')\n",
    "\n",
    "#img to histogram\n",
    "img_hst = image_test.histogram()\n",
    "\n",
    "#seperate rgb values from histogram to plot seperately\n",
    "def getRed(R): \n",
    "    return '#%02x%02x%02x'%(R,0,0)\n",
    "def getGreen(G): \n",
    "    return '#%02x%02x%02x'%(0,G,0)\n",
    "def getBlue(B):\n",
    "    return '#%02x%02x%02x'%(0,0,B)\n",
    "\n",
    "#iterate through img histogram to find rgb values\n",
    "Red=img_hst[0:256] # indicates Red\n",
    "Green=img_hst[256:512] # indicated Green\n",
    "Blue=img_hst[512:768] # indicates Blue\n",
    "\n",
    "for i in range(0, 256):\n",
    "    plt.bar(i, Red[i], color = getRed(i),alpha=1)\n",
    "    plt.figure(1) # plot red\n",
    "for i in range(0, 256):\n",
    "    plt.bar(i, Green[i], color = getGreen(i),alpha=0.3)\n",
    "    plt.figure(2) # plot green\n",
    "for i in range(0, 256):\n",
    "    plt.bar(i, Blue[i], color = getBlue(i),alpha=0.3)\n",
    "    plt.figure(3) #plot blue\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for data cleaning and preparation:\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# us_cities = pd.read_csv(\"uscities_clean.csv\")\n",
    "# us_cities = us_cities.drop_duplicates(subset='city', keep='first')\n",
    "# us_cities = us_cities.reset_index(drop=True)\n",
    "# #19,091 cities stored\n",
    "\n",
    "# states = pd.read_csv(\"state_capitals.csv\")\n",
    "# capitals = [_ for _ in states['Capital']]\n",
    "\n",
    "# is_capital = [city in capitals for city in us_cities['city']]\n",
    "# us_cities['capital'] = is_capital\n",
    "\n",
    "# us_cities['zips'] = [len(str.split(str(_))) for _ in us_cities['zips']]\n",
    "# us_cities\n",
    "\n",
    "# Manually included Washington DC as (quasi-)\"State Capital\"\n",
    "# Corrected 'St. Paul' to 'Saint Paul' to be detected by script\n",
    "\n",
    "# us_cities.to_csv(r'...\\SatteliteAnalysis\\us_cities_v2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ethical considerations:**\n",
    "\n",
    "Regarding our datasets, there is an uncertain degree of bias that affects the process of taking satellite imagery. Perhaps some regions are more “photogenic” or are more likely to have specific documented events such as natural disasters or the like. Considering this, we should be mindful that our interpretations may be influenced by imaging bias or analytical bias. If a similar analysis to this project was used for developing maps or for disaster prevention efforts, there could be a wide range of consequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Processing:**\n",
    "\n",
    "Utilizing geo.json coordinates to load parameters for api queries. By using different location sets we can find significant locations to test. By normalizing the query data we can train a test model to recognize certain types of images based on different parameters. Parameters can be based on expected attributes of trained datasets.\n",
    "\n",
    "As we collect our data, we will need to process images, sort visual findings, and store our conclusions. We plan to use built-in python packages to visualize our results (i.e., see trends in data, and portray model fitments). These packages include, but are not limited to, NumPy, SciPy, SciKit Learn, Matplotlib, and NASA’s Bingo. Data Processing will be the crux of this project and allow us to come to many conclusions about weather patterns, climate change, and other geographic changes that we might see. Skills we have learned throughout this semester will aid in these tasks.\n",
    "\n",
    "Allocating the appropriate metadata to allow us to make hypothetical predictions about changes occurring due to features outside of the received information. By categorizing locational data appropriately, we can classify regions by qualities outside of just its specific location. The features found in regions with rockier terrain or in areas affected by changes in temperature are all valuable classifiers that we can make predictions and base models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example query from a previous date\n",
    "url = \"https://api.nasa.gov/planetary/earth/imagery?lon=111.10&lat=39.01&dim=.25&date=2019-03-26&api_key=II3XUxmGYfFKmyapNeXjrH3Os3ILNhQScjW6bbMT\"\n",
    "response = requests.get(url)\n",
    "\n",
    "#image returned for comparison \n",
    "i2 = Image.open(BytesIO(response.content))\n",
    "\n",
    "#plotted and by using our dimendions we can measure distances and scale accurately\n",
    "imgplot = plt.imshow(i2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Analysis:**\n",
    "\n",
    "As outlined in our Project Objectives, metadata associated with each image may be an essential way to navigate, categorize, and classify the items within our dataset. Doing preliminary analysis with the metadata might reveal relationships we want to explore further, deter us from other approaches, and act as a foundation for any further studies involved with image processing. \n",
    "    \n",
    "By taking image data from the same location over a given period of time, we will be able to identify changes in location regardless of any temporal factors. Pixel analysis to a heat map can show change in color, or with the right masking tools change in geographical features and or cloud coverage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Methodology:**\n",
    "\n",
    "Cross references our datasets against normalized test data, to ensure results our results are within the expected range. Sampling random queries can allow us to double check and validate before we make any assertions based on false data processing. We want to ensure as much accuracy as possible, by allocating the appropriate stopgaps in our data analysis we can maintain some control over any potential analysis inaccuracies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Schedule:**\n",
    "- March 26th - Project Proposal Due\n",
    "- April 6th - Target for Cleaned/Explored Dataset\n",
    "- April 11th - Project Milestone Due\n",
    "- April 19th - Target for Completed Screen-Cast Presentation\n",
    "- April 25th - Final Project Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources:**\n",
    "Geo.json of U.S. cities containing population, location, and more features: https://simplemaps.com/data/us-cities\n",
    ".csv containing U.S. state capitals and state features: https://www.downloadexcelfiles.com/us_en/download-excel-file-list-state-capitals-united-states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
